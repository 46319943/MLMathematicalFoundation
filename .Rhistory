2*exp(-2 * 0.3^2 * 10)
10!
factorial(10)
factorial(10)/(factorial(5)*factorial(5))
0.5^10
0.5^10 * 252
factorial(10)/(factorial(1)*factorial(9))
10 * (0.9^9 * 0.1)
0.1^10 + (0.1^9 * 0.9) * 10
2 * exp(-2 * 10 * 0.8^2)
12^5
2^5
256/32
1/6
pbeta(0.5, 1, 5)
qbeta(0.975, 8, 16)
qbeta(0.975, 8, 16, lower.tail = FALSE)
qbeta(0.025, 8, 16)
pbeta(0.35, 8, 16)
pbeta(0.35, 8, 21)
theta = range(0, 1, 0.01)
plot(theta, pbeta(theta, 1, 6))
pbeta(theta, 1, 6)
theta
range(0, 1, 0.01)
theta = seq(0, 1, 0.01)
plot(theta, pbeta(theta, 1, 6))
plot(theta, pbeta(theta, 1, 6), type='l')
plot(theta, dbeta(theta, 1, 6), type='l')
67/6
qbeta(0.05, 67, 6)
qgamma(0.05, 67, 6)
factor(5)
factorial(5)
factorial(4)*factorial(3)*factorial(3)/(
factorial(5)*factorial(2)*factorial(2)
)
factorial(4)*factorial(3)*factorial(3)
factorial(5)*factorial(2)*factorial(2)
factorial(4)*factorial(3)*factorial(2)/(
factorial(5)*factorial(2)*factorial(2)
)
factorial(14)*factorial(6)*factorial(9)/(
factorial(15)*factorial(5)*factorial(9)
)
factorial(3)
factorial(5)
gamma(5)
gamma(6)
gamma(4)*gamma(3)*gamma(2)/(
+     gamma(5)*gamma(2)*gamma(2)
+ )
gamma(4)*gamma(3)*gamma(2)/(
gamma(5)*gamma(2)*gamma(2)
)
gamma(14)*gamma(6)*gamma(9)/(
gamma(15)*gamma(5)*gamma(9)
)
(1/20)/(1/16)
1/4
0.25/0.8
20/16
20+12+15+8+13.5+25
93.5/6
6/93.5
pgamma(0.1, 6, 93.5)
30+16+8+114+60+4+23+30+105
qgamma(0.975, 9, 390)
f = function(){}
f = function(alpha, beta, y){return(
beta^alpha * alpla / (beta+y)^(alpha+1)
)}
seq(0, 200)
y = seq(0, 200)
f(9, 390, y)
f = function(alpha, beta, y){return(
beta^alpha * alpha / (beta+y)^(alpha+1)
)}
f(9, 390, y)
plot(y, f(9, 390, y), type='l')
y
f(9, 390, y)
plot(y, f(9, 390, y), type='l')
y=seq(0, 120)
y=seq(0, 120)
plot(y, f(9, 390, y), type='l')
20/16
(94.6+95.4+96.2+94.9+95.9)/5
5*95.4/0.25
1/24
(1908+400)/(20+4)
qnorm(0.975, 96.17, 0.042)
pnorm(100, 96.17, 0.042)
pnorm(50, 96.17, 0.042)
pnorm(96.17, 96.17, 0.042)
z <- rgamma(n=1000, shape=a, rate=b)
x <- 1/z
z <- rgamma(n=1000, shape=3, rate=200)
x <- 1/z
plot(x)
x
plot x
plot(x)
mean(x)
read.table("http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
dat <- read.table("http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
View(dat)
View(dat)
datF <- subset(dat, FM==1, select=1:2)
datF <- subset(dat, V3==1, select=1:2)
datM <- subset(dat, V3==2, select=1:2)
plot(datF)
plot(datM)
datF.lm=lm(datF$V1~datF$V2)
summary(datF.lm)
predict(datF.lm, data.frame(V1=260),interval="predict")
predict(datF.lm, data.frame(V1=260),interval="predict")
predict(datF.lm, interval="predict")
predict(datF.lm)
predict(datF.lm, data.frame(V1=260),interval="predict")
?predict
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
datF.lm=lm(V1~V2, data=datF)
summary(datF.lm)
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
predict(datF.lm, newdata = data.frame(V1=260, V2=0),interval="predict")
datF.lm=lm(V2~V1, data=datF)
summary(datF.lm)
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
dat$V3==1
dat[dat$V3==1]
dat(dat$V3==1])
dat(dat$V3==1)
dat$V1[dat$V3==1]
dat$V1[dat$V3==1] <- 0
dat=read.table(http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
dat=read.table("http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
dat$V3[dat$V3==1] <- 0
dat$V3[dat$V3==2] <- 1
pairs(dat)
lm(V2~V1+V3, data = dat)
model = lm(V2~V1+V3, data = dat)
summary(model)
plot(fitted(model), residuals(model))
rgamma(10000, 5, 3)
theta = rgamma(10000, 5, 3)
result = theta / (1 - theta)
mean(result)
theta
mean(result)
result
mean(result)
result > 1
mean(result > 1)
theta = rbeta(10000, 5, 3)
result = theta / (1 - theta)
mean(result)
mean(result > 1)
n = rnorm(10000, 0, 1)
quantile(n, 0.3)
quantile(n)
sqrt(5.2/5000)
Q = matrix(c(0.0, 0.5,
0.5, 0.0,),
nrow=2, byrow=TRUE)
Q = matrix(c(0.0, 0.5,
0.5, 0.0),
nrow=2, byrow=TRUE)
Q = matrix(c(0.0, 1.0,
0.3, 0.7),
nrow=2, byrow=TRUE)
Q^4
Q %*% Q %*% Q %*% Q
Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q
Q %*% Q %*% Q
lg = function(mu, n, ybar) {
mu2 = mu^2
n * (ybar * mu - mu2 / 2.0) - log(1 + mu2)
}
mh = function(n, ybar, n_iter, mu_init, cand_sd) {
## Random-Walk Metropolis-Hastings algorithm
## step 1, initialize
mu_out = numeric(n_iter)
accpt = 0
mu_now = mu_init
lg_now = lg(mu=mu_now, n=n, ybar=ybar)
## step 2, iterate
for (i in 1:n_iter) {
## step 2a
mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate
## step 2b
lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate
lalpha = lg_cand - lg_now # log of acceptance ratio
alpha = exp(lalpha)
## step 2c
u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)
if (u < alpha) { # then accept the candidate
mu_now = mu_cand
accpt = accpt + 1 # to keep track of acceptance
lg_now = lg_cand
}
## collect results
mu_out[i] = mu_now # save this iteration's value of mu
}
## return a list of output
list(mu=mu_out, accpt=accpt/n_iter)
}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(43) # set the random seed for reproducibility
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)
str(post)
library("coda")
traceplot(as.mcmc(post$mu))
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.05)
post$accpt
traceplot(as.mcmc(post$mu))
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.9)
post$accpt
traceplot(as.mcmc(post$mu))
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=30.0, cand_sd=0.9)
post$accpt
traceplot(as.mcmc(post$mu))
post$mu_keep = post$mu[-c(1:100)] # discard the first 200 samples
plot(density(post$mu_keep, adjust=2.0), main="", xlim=c(-1.0, 3.0), xlab=expression(mu)) # plot density estimate of the posterior
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(ybar, 0, pch=19) # sample mean
curve(0.017*exp(lg(mu=x, n=n, ybar=ybar)), from=-1.0, to=3.0, add=TRUE, col="blue") # approximation to the true posterior in blue
density(post$mu_keep, adjust=2.0), main="", xlim=c(-1.0, 3.0), xlab=expression(mu)
density(post$mu_keep, adjust=2.0)
update_mu = function(n, ybar, sig2, mu_0, sig2_0) {
sig2_1 = 1.0 / (n / sig2 + 1.0 / sig2_0)
mu_1 = sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)
rnorm(n=1, mean=mu_1, sd=sqrt(sig2_1))
}
update_sig2 = function(n, y, mu, nu_0, beta_0) {
nu_1 = nu_0 + n / 2.0
sumsq = sum( (y - mu)^2 ) # vectorized
beta_1 = beta_0 + sumsq / 2.0
out_gamma = rgamma(n=1, shape=nu_1, rate=beta_1) # rate for gamma is shape for inv-gamma
1.0 / out_gamma # reciprocal of a gamma random variable is distributed inv-gamma
}
gibbs = function(y, n_iter, init, prior) {
ybar = mean(y)
n = length(y)
## initialize
mu_out = numeric(n_iter)
sig2_out = numeric(n_iter)
mu_now = init$mu
## Gibbs sampler
for (i in 1:n_iter) {
sig2_now = update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)
mu_now = update_mu(n=n, ybar=ybar, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)
sig2_out[i] = sig2_now
mu_out[i] = mu_now
}
cbind(mu=mu_out, sig2=sig2_out)
}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)
## prior
prior = list()
prior$mu_0 = 0.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(53)
init = list()
init$mu = 0.0
post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)
summary(as.mcmc(post))
summary(post)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
ybar = mean(y)
n = length(y)
## prior
prior = list()
prior$mu_0 = 0.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(53)
init = list()
init$mu = 0.0
post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)
summary(as.mcmc(post))
summary(post)
## prior
prior = list()
prior$mu_0 = 1.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(53)
init = list()
init$mu = 0.0
post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)
summary(post)
52 + 25 + 263
340 + 269.8
696+279+169.1+127.9+119.6+399+33+25
696+279+169.1+127.9+119.6+499+33+25
397+261
126.83 + 19.9 + 399 + 499 + 689
1948.6 + 261
2391.73+1948.6
2391.73+1948.6 / 2
(2391.73+1948.6) / 2
2391.73 - 2170.165
658 - 261
2391.73 - 261
1948.60 +261
696+279+169.1+127.9+119.6+499+33+25
696+279+169.1+127.9+119.6+449+33+25
2130.73 + 2159.6 + 2142 + 609.8
7509.93 - 7042.13
549+397+3485.6+178.83+2411.8+19.9
2130.73 + 2159.6
2142 + 609.8
467.8 - 2698
467.8 - 269.8
198
exp(-0.317466267006714)
0.3*2 + 0.45*3 + 0.25*0.5
0.2^2
0.3*(2 + 2^2) + 0.45*(3 + 3^2) + 0.25*(0.5 + 0.5^2) - (2.075)^2
43137 + 1731 + 12166 + 97 + 1217 + 1375 + 2939 + 2428 + 486
43137 + 1731 + 12166 + 97 + 1217 + 1375 + 2939 + 2428 + 486 + 16238
setwd('C:/Document/MLMathematicalFoundation')
m = 100
epsilon = 0.1
delta = 0.05
target = function(x){
return(
2 * m * exp(-2 * epsilon^2 * x) - delta
)
}
library('nleqslv')
nleqslv(100, target)
